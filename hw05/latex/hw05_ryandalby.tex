\documentclass[12pt]{article}

\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.9in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\topmargin}{0.05in}
\setlength{\headheight}{-0.05in}
\setlength{\headsep}{0.0in}
\renewcommand*\arraystretch{1.3}


\begin{document}

\begin{center}
{\bf CS 6300} \hfill {\large\bf HW05: TD and Q Learning} \hfill {\bf Ryan Dalby} \hfill {\bf Due March 3, 2022}
\end{center}

\noindent
Please use the \LaTeX\ template to produce your writeups. See the
Homework Assignments page on the class website for details.  Hand in
via gradescope.

Jennifer is currently finishing up in college and is trying to decide
what she wants to do with the rest of her life.  She assumes her
options can be modeled as an MDP, with a discount $\gamma=1/2$.  At
each point in her life she can choose to either continue in school
(action $x$) or try to get a job (action $y$).  Her three states are
{\bf C}ollege, {\bf G}rad School and {\bf J}ob.  {\bf J} is a terminal
state.

Suppose Jennifer doesn't actually know the MDP model.  Instead, she
watched three of her older siblings go through life.  They exhibited
the following episodes.

\begin{center}
\begin{tabular}{|l|l|l|} \hline
{\bf Sibling 1} & {\bf Sibling 2} & {\bf Sibling 3} \\ \hline
    C, x, -200 & C, x, -400 & C, x, -400 \\
    C, x, -200 & G, x,  100 & G, x,  100 \\
    C, y,  400 & G, x,  100 & G, x,  100 \\
    J          & G, y, 1000 & G, x,  200 \\
               & J          & J          \\ \hline
\end{tabular}
\end{center}

  \begin{enumerate}

  \item What are the transition probabilities and rewards that you can
    know about?

  \begin{center}
  \begin{tabular}{|l|l|} \hline
    \textbf{Transition Probabilities} & \textbf{Rewards} \\ \hline
    T(C, x, C) & R(C, x, C) \\
    T(C, x, G) & R(C, x, G) \\
    T(G, x, G) & R(G, x, G) \\
    T(G, x, J) & R(G, x, J) \\
    T(C, y, J) & R(C, y, J) \\
    T(G, y, J) & R(G, y, J) \\ \hline
  \end{tabular}
  \end{center}

  \item Find the values $V(s)$ using direct estimation.

  \small{
  \[
    V(C) = \frac{(-200 + -100 + 100) + (-200 + 200) + (400) + (-400 + 50 + 25 + 125) + (-400 + 50 + 25 + 25)}{5}
  \]
  }
  \[
    V(C) = -60
  \]

  \[
    V(G) = \frac{(100+50+250) + (100+500) + (1000) + (100 + 50 + 50) + (100 + 100) + (200)}{6}
  \]
  \[
    V(G) = \frac{1300}{3} \approx 433.33
  \]


  \item Use TD Learning instead to find estimates of the values,
    assuming $\alpha=1/2^{n-1}$, where $n$ is the sibling number.
  
  Using updates as follows: 
  \[
    V^\pi(s) = (1-\alpha) V^\pi(s) + \alpha (R(s, a, s') + \gamma V^\pi(s'))
  \]

  \begin{center}
    \begin{tabular}{lll} \hline
      \textbf{Episode} & \textbf{Trial} & \textbf{Update} \\ \hline
      1 & 1 & $V^\pi(C) = 0 + 1(-200 + \frac{1}{2}(0)) = -200$ \\
      & 2 & $V^\pi(C) = 0 + 1(-200 + \frac{1}{2}(-200)) = -300$ \\
      & 3 & $V^\pi(C) = 0 + 1(400 + \frac{1}{2}(0)) = 400$ \\ \hline
      2 & 1 & $V^\pi(C) = \frac{1}{2} (400) + \frac{1}{2}(-400 + \frac{1}{2}(0)) = 0$ \\
      & 2 & $V^\pi(G) = \frac{1}{2} (0) + \frac{1}{2}(100 + \frac{1}{2}(0)) = 50$ \\
      & 3 & $V^\pi(G) = \frac{1}{2} (50) + \frac{1}{2}(100 + \frac{1}{2}(50)) = \frac{175}{2} = 87.5$ \\
      & 4 & $V^\pi(G) = \frac{1}{2} (\frac{175}{2}) + \frac{1}{2}(1000 + \frac{1}{2}(0)) = \frac{2175}{4} = 543.75$ \\ \hline
      3 & 1 & $V^\pi(C) = \frac{3}{4} (0) + \frac{1}{4}(-400 + \frac{1}{2}(\frac{2175}{4})) = -\frac{1025}{32} \approx -32.03$ \\
      & 2 & $V^\pi(G) = \frac{3}{4} (\frac{2175}{4}) + \frac{1}{4}(100 + \frac{1}{2}(\frac{2175}{4})) = \frac{16025}{32} \approx 500.78$ \\
      & 3 & $V^\pi(G) = \frac{3}{4} (\frac{16025}{32}) + \frac{1}{4}(100 + \frac{1}{2}(\frac{16025}{32})) = \frac{118575}{256} \approx 463.18$ \\
      & 4 & $V^\pi(G) = \frac{3}{4} (\frac{118575}{256}) + \frac{1}{4}(200 + \frac{1}{2}(0)) = \frac{406925}{1024} \approx 397.39$ \\

      
    \end{tabular}
  \end{center}

  \item Use Q learning instead, and extract the estimated optimal policy.

  Using updates as follows: 
  \[
    Q(s,a) = (1-\alpha) Q(s,a) + \alpha (R(s, a, s') + \gamma \max_{a'} Q(s', a'))
  \]

  \begin{center}
    \begin{tabular}{lll} \hline
      \textbf{Episode} & \textbf{Trial} & \textbf{Update} \\ \hline
      1 & 1 & $Q(C,x) = 0 + 1(-200 + \frac{1}{2}(\max\{0, 0\})) = -200$ \\ 
      & 2 & $Q(C,x) = 0 + 1(-200 + \frac{1}{2}(\max\{-200, 0\})) = -200$ \\
      & 3 & $Q(C,y) = 0 + 1(400 + \frac{1}{2}(0)) = 400$ \\ \hline
      2 & 1 & $Q(C,x) = \frac{1}{2}(-200) + \frac{1}{2}(-400 + \frac{1}{2}(\max\{0, 0\})) = -300$ \\ 
      & 2 & $Q(G,x) = \frac{1}{2}(0) + \frac{1}{2}(100 + \frac{1}{2}(\max\{0, 0\})) = 50$ \\ 
      & 3 & $Q(G,x) = \frac{1}{2}(50) + \frac{1}{2}(100 + \frac{1}{2}(\max\{50, 0\})) = \frac{175}{2} = 87.5$ \\ 
      & 4 & $Q(G,y) = \frac{1}{2}(0) + \frac{1}{2}(1000 + \frac{1}{2}(0)) = 500$ \\ \hline
      3 & 1 & $Q(C,x) = \frac{3}{4}(-300) + \frac{1}{4}(-400 + \frac{1}{2}(\max\{\frac{175}{2}, 500\})) = -\frac{525}{2} = -262.5$ \\ 
      & 2 & $Q(G,x) = \frac{3}{4}(\frac{175}{2}) + \frac{1}{4}(100 + \frac{1}{2}(\max\{\frac{175}{2}, 500\})) = \frac{1225}{8} \approx 153.12$ \\ 
      & 3 & $Q(G,x) = \frac{3}{4}(\frac{1225}{8}) + \frac{1}{4}(100 + \frac{1}{2}(\max\{\frac{1225}{8}, 500\})) = \frac{6475}{32} \approx 202.34$ \\ 
      & 4 & $Q(G,x) = \frac{3}{4}(\frac{6475}{32}) + \frac{1}{4}(200 + \frac{1}{2}(0)) = \frac{25825}{128} \approx 201.76$ \\ 
    \end{tabular}
  \end{center}

  The optimal policy is:
  \[
    \pi(C) = y
  \]
  \[
    \pi(G) = y
  \]

  \end{enumerate}

\end{document}
